{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Environment: A Simple GridWorld\n",
    "Let's create a 3x4 grid. The agent's goal is to get from a starting position to one of the two terminal states.\n",
    "\n",
    "Grid: 3 rows, 4 columns.\n",
    "\n",
    "States: 12 states (0-11).\n",
    "\n",
    "Actions: 0: up, 1: down, 2: left, 3: right.\n",
    "\n",
    "Start State: State 8 (bottom-left).\n",
    "\n",
    "Terminal States:\n",
    "\n",
    "State 3 (top-right) with a reward of +10.\n",
    "\n",
    "State 7 (middle-right) with a reward of -10 (a cliff).\n",
    "\n",
    "Walls: State 5 is an obstacle the agent cannot enter.\n",
    "\n",
    "Reward: Every non-terminal step gives a small negative reward of -0.1 to encourage finding the goal faster."
   ],
   "id": "d93b49069acdb077"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    A simple GridWorld environment for the agent.\n",
    "    This class provides the mechanics of the world (how to move, what are the rewards),\n",
    "    but it does NOT expose the transition probabilities P(s'|s,a).\n",
    "    The agent must learn them through experience.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Define grid dimensions\n",
    "        self.rows = 3\n",
    "        self.cols = 4\n",
    "        self.num_states = self.rows * self.cols\n",
    "        self.num_actions = 4  # 0: up, 1: down, 2: left, 3: right\n",
    "\n",
    "        # Define special states\n",
    "        self.start_state = 8\n",
    "        self.goal_state = 3\n",
    "        self.trap_state = 7\n",
    "        self.wall_state = 5\n",
    "\n",
    "        self.terminal_states = [self.goal_state, self.trap_state]\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"\n",
    "        Perform an action in a given state.\n",
    "        Returns: (next_state, reward, done)\n",
    "        \"\"\"\n",
    "        if state in self.terminal_states:\n",
    "            return state, 0, True\n",
    "\n",
    "        row, col = state // self.cols, state % self.cols\n",
    "\n",
    "        # Calculate next position based on action\n",
    "        if action == 0:  # up\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 1:  # down\n",
    "            row = min(row + 1, self.rows - 1)\n",
    "        elif action == 2:  # left\n",
    "            col = max(col - 1, 0)\n",
    "        elif action == 3:  # right\n",
    "            col = min(col + 1, self.cols - 1)\n",
    "\n",
    "        next_state = row * self.cols + col\n",
    "\n",
    "        # Agent cannot enter the wall, so it stays in place\n",
    "        if next_state == self.wall_state:\n",
    "            next_state = state\n",
    "\n",
    "        # Define rewards\n",
    "        if next_state == self.goal_state:\n",
    "            reward = 10\n",
    "        elif next_state == self.trap_state:\n",
    "            reward = -10\n",
    "        else:\n",
    "            reward = -0.1 # Small cost for each move\n",
    "\n",
    "        done = next_state in self.terminal_states\n",
    "        return next_state, reward, done"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**MC Basic Algorithm Implementation**\n",
    "\n",
    "This is a direct, though inefficient, implementation of the pseudocode. It iterates through every state-action pair and generates many episodes starting from that pair to evaluate its Q-value."
   ],
   "id": "7cacc66290537da0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def mc_basic(env, num_iterations, episodes_per_pair, gamma=0.99):\n",
    "\n",
    "    # Initialize Q-values and returns\n",
    "    policy = {s: random.randint(0, env.num_actions - 1) for s in range(env.num_states)}\n",
    "    Q = defaultdict(float) # Q[s][a] = 0.0\n",
    "\n",
    "    # Main Loop for k-th iteration\n",
    "    for k in range(num_iterations):\n",
    "        print(f'--- Iteration {k + 1}/{num_iterations} ---')\n",
    "\n",
    "        # --- Policy evaluation ---\n",
    "        for s in range(env.num_states):\n",
    "            if s in env.terminal_states or s == env.wall_state:\n",
    "                continue\n",
    "            for a in range(env.num_actions):\n",
    "                returns_for_pair = []\n",
    "                for _ in range(episodes_per_pair):\n",
    "                    episode_return = 0\n",
    "                    t = 0\n",
    "\n",
    "                    current_state, reward, done = env.step(s, a)\n",
    "                    episode_return += reward\n",
    "\n",
    "                    # Following the policy pi_k for the rest of the episode\n",
    "                    while not done:\n",
    "                        action_from_policy = policy[current_state]\n",
    "                        next_state, reward, done = env.step(current_state, action_from_policy)\n",
    "                        episode_return += (gamma ** (t+1)) * reward\n",
    "                        current_state = next_state\n",
    "                        t += 1\n",
    "                    returns_for_pair.append(episode_return)\n",
    "\n",
    "                # q_pi_k(s, a) = average of returns for (s, a)\n",
    "                if returns_for_pair:\n",
    "                    Q[(s, a)] = np.mean(returns_for_pair)\n",
    "\n",
    "        # --- Policy improvement ---\n",
    "        for s in range(env.num_states):\n",
    "            if s in env.terminal_states or s == env.wall_state:\n",
    "                continue\n",
    "\n",
    "            # Find the best action for state s\n",
    "            q_values_for_s = [Q[(s, a)] for a in range(env.num_actions)]\n",
    "\n",
    "        best_action = np.argmax(q_values_for_s)\n",
    "        # Update policy\n",
    "        policy[s] = best_action\n",
    "\n",
    "    print('--- MC Basic Finished ---')\n",
    "    return policy, Q"
   ],
   "id": "f40fd47363e3d018",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mc_basic(env=GridWorld(), num_iterations=50, episodes_per_pair=50, gamma=0.99)",
   "id": "6468b451bf85f68b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**MC Exploring Starts Implementation**\n",
    "\n",
    "This version is much more efficient. It generates one episode at a time (with a random starting pair) and updates the Q-values and policy for all state-action pairs visited in that episode."
   ],
   "id": "a73674700079269d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def mc_exploring_starts(env, num_episodes, gamma=0.99, max_steps_per_episode=5):\n",
    "\n",
    "    # Initialize Q-values and returns\n",
    "    policy = {s: 0 for s in range(env.num_states)}\n",
    "    Q = defaultdict(float)  # Q[s][a] = 0.0\n",
    "    Returns = defaultdict(float)  # Returns[s][a] = []\n",
    "    Num = defaultdict(int)  # Num[s][a] = 0\n",
    "\n",
    "    # For each episode\n",
    "    for i in range(num_episodes):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f'--- Episode {i + 1}/{num_episodes} ---')\n",
    "\n",
    "        # --- Episode Generation ---\n",
    "        # Randomly select a starting state and action\n",
    "        s0 = random.randint(0, env.num_states - 1)\n",
    "        while s0 in env.terminal_states or s0 == env.wall_state:\n",
    "            s0 = random.randint(0, env.num_states - 1)\n",
    "        a0 = random.randint(0, env.num_actions - 1)\n",
    "\n",
    "        current_state = s0\n",
    "        current_action = a0\n",
    "\n",
    "        episode = []\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        # Generate the episode of length T\n",
    "        while not done and steps < max_steps_per_episode:\n",
    "            next_state, reward, done = env.step(current_state, current_action)\n",
    "            episode.append((current_state, current_action, reward))\n",
    "            current_state = next_state\n",
    "\n",
    "            if not done:\n",
    "                current_action = policy[current_state]  # Follow the policy for the next action\n",
    "\n",
    "        # --- Policy Evaluation and Episode Improvement for Each Step ---\n",
    "        g = 0  # Initialize g, the return\n",
    "        visited_pairs = set()\n",
    "\n",
    "        # For each step of the episode, t = T-1, T-2, ..., 0\n",
    "        for t in range(len(episode) -1, -1, -1):\n",
    "            s_t, a_t, r_t_plus_1 = episode[t]\n",
    "            g = gamma * g + r_t_plus_1\n",
    "\n",
    "            # We use First Visit MC\n",
    "            if (s_t, a_t) not in visited_pairs:\n",
    "                visited_pairs.add((s_t, a_t))\n",
    "\n",
    "                Returns[(s_t, a_t)] += g\n",
    "                Num[(s_t, a_t)] += 1\n",
    "\n",
    "                Q[(s_t, a_t)] = Returns[(s_t, a_t)] / Num[(s_t, a_t)]\n",
    "\n",
    "                q_values_for_s_t = [Q[(s_t, a)] for a in range(env.num_actions)]\n",
    "                policy[s_t] = np.argmax(q_values_for_s_t)\n",
    "\n",
    "    print('--- MC Exploring Starts Finished ---')\n",
    "    return policy, Q"
   ],
   "id": "c92c858eb9a7b40b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mc_exploring_starts(env=GridWorld(), num_episodes=5, gamma=0.99, max_steps_per_episode=5)",
   "id": "9e644261307eb184",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**MC ε-Greedy Implementation**\n",
    "\n",
    "This is the most practical of the three. It removes the need for exploring starts by using an ε-greedy policy, which ensures continuous exploration."
   ],
   "id": "990c1d0b18a12b5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " def mc_epsilon_greedy(env, num_episodes, gamma=0.99, epsilon=0.1):\n",
    "\n",
    "    # Initialize Q-values and returns\n",
    "    Q = defaultdict(float)\n",
    "    Returns = defaultdict(float)\n",
    "    Num = defaultdict(int)\n",
    "    policy = {s: np.ones (env.num_actions) / env.num_actions for s in range(env.num_states)}\n",
    "\n",
    "    # For each episode, do:\n",
    "    for i in range(num_episodes):\n",
    "        #if (i + 1) % 1000 == 0:\n",
    "        #    print(f'--- Episode {i + 1}/{num_episodes} ---')\n",
    "\n",
    "        # --- Episode Generation ---\n",
    "        current_state = env.start_state\n",
    "        episode = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Choose action using ε-greedy policy\n",
    "            action = np.random.choice(np.arange(env.num_actions), p = policy[current_state])\n",
    "\n",
    "            next_state, reward, done = env.step(current_state, action)\n",
    "            episode.append((current_state, action, reward))\n",
    "            current_state = next_state\n",
    "\n",
    "        # --- Update Q and Policy for each first-visit pairin the episode ---\n",
    "        g = 0\n",
    "        visited_pairs = set()\n",
    "\n",
    "        # For each step of the episode, t = T-1, T-2, ..., 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            s_t, a_t, r_t_plus_1 = episode[t]\n",
    "            g = gamma * g + r_t_plus_1\n",
    "\n",
    "            if (s_t, a_t) not in visited_pairs:\n",
    "                visited_pairs.add((s_t, a_t))\n",
    "\n",
    "                Returns[(s_t, a_t)] += g\n",
    "                Num[(s_t, a_t)] += 1\n",
    "                Q[(s_t, a_t)] = Returns[(s_t, a_t)] / Num[(s_t, a_t)]\n",
    "\n",
    "                # Policy Improvement for state s_t\n",
    "                best_action = np.argmax([Q[(s_t, a)] for a in range(env.num_actions)])\n",
    "                # Update policy probabilities for state s_t\n",
    "                for a in range(env.num_actions):\n",
    "                    if a == best_action:\n",
    "                        policy[(s_t, a)] = 1 - epsilon + (epsilon / env.num_actions)\n",
    "                    else:\n",
    "                        policy[(s_t, a)] = epsilon / env.num_actions\n",
    "\n",
    "    print('--- MC ε-Greedy Finished ---')\n",
    "    return policy, Q\n",
    "\n"
   ],
   "id": "658627fd9f9919e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "59d12e801acbebbf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
